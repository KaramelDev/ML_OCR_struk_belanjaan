{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf681e5a",
   "metadata": {},
   "source": [
    "ini jadi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155a178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from fuzzywuzzy import fuzz\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import util\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "import tempfile\n",
    "import nest_asyncio\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e07f98",
   "metadata": {},
   "source": [
    "ambil data pusat dari postgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51356816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_pusat():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"strukAI\",\n",
    "        user=\"postgres\",\n",
    "        password=\"bairn1021\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    df = pd.read_sql_query(\"SELECT * FROM data_pusat\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    df = df.dropna()\n",
    "    descriptions = df[\"Description\"].astype(str).str.lower().tolist()\n",
    "    ids = df[\"id\"].tolist()\n",
    "    prices = df[\"Harga Jual ke Konsumen yg Disarankan\"].tolist()\n",
    "\n",
    "    return df, descriptions, ids, prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c57f3",
   "metadata": {},
   "source": [
    "OCR menggunakan Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660edbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyCG8IGd5lgD4m2UocqUGOyGtyd0jM6O4vU\")\n",
    "model_gemini = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "def OCR_Gemini(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    prompt = \"\"\"\n",
    "    Ini adalah struk belanja. Tolong ekstrak informasinya dalam format JSON dengan field:\n",
    "    - invoice_number (string)\n",
    "    - phone (string)\n",
    "    - alamat (string)\n",
    "    - email (string)\n",
    "    - nama_toko (string)\n",
    "    - tanggal (string, format DD/MM/YYYY)\n",
    "    - daftar_barang (array of objects: nama, qty, harga_satuan, subtotal)\n",
    "    - total_belanja (number)\n",
    "    Jika ada informasi yang tidak jelas, isi dengan null.\n",
    "    Hanya kembalikan JSON-nya saja, tanpa penjelasan atau markdown formatting.dalam lowercase\n",
    "    \"\"\"\n",
    "    response = model_gemini.generate_content([prompt, img])\n",
    "    text = response.text\n",
    "    cleaned = re.sub(r'^```json|```$', '', text, flags=re.MULTILINE).strip()\n",
    "    return json.loads(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370635ac",
   "metadata": {},
   "source": [
    "Fungsi untuk menentukan kesamaan data hasil OCR dengan data yang ada di pusat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "778a68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_items(items, descriptions, ids, prices, threshold=0.59):\n",
    "    result = []\n",
    "\n",
    "    for item in items:\n",
    "        prompt = f\"\"\"\n",
    "        Aku memiliki data produk sebagai berikut:\n",
    "        {descriptions}\n",
    "\n",
    "        Dari daftar produk di atas, coba cocokan dengan barang berikut:\n",
    "        \"{item['nama']}\"\n",
    "\n",
    "        Jika ada kecocokan, kembalikan dalam format JSON:\n",
    "        {{\n",
    "            \"id\": id_produk,\n",
    "            \"nama\": \"nama_produk\",\n",
    "            \"confidence\": tingkat_kepercayaan (0-1)\n",
    "        }}\n",
    "        Jika tidak ada kecocokan, balas:\n",
    "        {{\n",
    "            \"id\": null,\n",
    "            \"nama\": null,\n",
    "            \"confidence\": 0\n",
    "        }}\n",
    "        \"\"\"\n",
    "        response = model_gemini.generate_content(prompt)\n",
    "        text = response.text\n",
    "        cleaned = re.sub(r'^```json|```$', '', text, flags=re.MULTILINE).strip()\n",
    "        try:\n",
    "            match_data = json.loads(cleaned)\n",
    "        except:\n",
    "            match_data = {\"id\": None, \"nama\": None, \"confidence\": 0}\n",
    "\n",
    "        confidence = match_data.get('confidence', 0)\n",
    "\n",
    "        # Normalize confidence if it's a list\n",
    "        if isinstance(confidence, list) and confidence:\n",
    "            confidence = confidence[0]\n",
    "        elif not isinstance(confidence, float):\n",
    "            confidence = 0\n",
    "\n",
    "        if confidence >= threshold:\n",
    "            result.append({\n",
    "                \"id\": match_data.get('id'),\n",
    "                \"name\": match_data.get('nama'),\n",
    "                \"ocr_result\": {\n",
    "                    \"name\": item['nama'],\n",
    "                    \"quantity\": float(item['qty']),\n",
    "                    \"price\": float(item['harga_satuan']),\n",
    "                    \"total\": float(item['subtotal']),\n",
    "                    \"accuration\": round(confidence, 4)\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            result.append({\n",
    "                \"id\": None,\n",
    "                \"name\": None,\n",
    "                \"ocr_result\": {\n",
    "                    \"name\": item['nama'],\n",
    "                    \"quantity\": float(item['qty']),\n",
    "                    \"price\": float(item['harga_satuan']),\n",
    "                    \"total\": float(item['subtotal']),\n",
    "                    \"accuration\": round(confidence, 4)\n",
    "                }\n",
    "            })\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e5b3c",
   "metadata": {},
   "source": [
    "Menentukan type Outout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777e9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRResult(BaseModel):\n",
    "    name: str | None\n",
    "    quantity: float | None\n",
    "    price: float | None\n",
    "    total: float | None\n",
    "    accuration: float | None\n",
    "\n",
    "class ItemMatched(BaseModel):\n",
    "    id: int| None\n",
    "    name:str | None\n",
    "    ocr_result: OCRResult\n",
    "\n",
    "class Merchant(BaseModel):\n",
    "    name: str | None\n",
    "    address:str | None\n",
    "    phone: str | None\n",
    "    email: str | None\n",
    "\n",
    "class FinalOutput(BaseModel):\n",
    "    invoice_number: str | None\n",
    "    tanggal: str | None\n",
    "    merchant: Merchant\n",
    "    items: List[ItemMatched]\n",
    "    grand_total: float | None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be033bd9",
   "metadata": {},
   "source": [
    "Set UP fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c025625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naeko\\AppData\\Local\\Temp\\ipykernel_15756\\2984853656.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(\"SELECT * FROM data_pusat\", conn)\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "df, descriptions, ids, prices = load_data_pusat()\n",
    "\n",
    "@app.post(\"/struk\", response_model=FinalOutput)\n",
    "async def struk(file: UploadFile = File(...)):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\") as tmp:\n",
    "        tmp.write(await file.read())\n",
    "        path = tmp.name\n",
    "\n",
    "    try:\n",
    "        OCRData = OCR_Gemini(path)\n",
    "        items = OCRData['daftar_barang']\n",
    "        matched = match_items(items, descriptions, ids, prices)  # Tanpa vectors dan model\n",
    "\n",
    "        result = {\n",
    "            \"invoice_number\": OCRData['invoice_number'],\n",
    "            \"tanggal\": OCRData['tanggal'],\n",
    "            \"merchant\": {\n",
    "                \"name\": OCRData['nama_toko'],\n",
    "                \"address\": OCRData['alamat'],\n",
    "                \"phone\": OCRData['phone'],\n",
    "                \"email\": OCRData['email']\n",
    "            },\n",
    "            \"items\": matched,\n",
    "            \"grand_total\": float(OCRData['total_belanja'])\n",
    "        }\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "@app.get(\"/\")\n",
    "def health_check():\n",
    "    return {\"status\": \"running\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a70f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [15756]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52240 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52240 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [15756]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c9443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naeko\\AppData\\Local\\Temp\\ipykernel_12016\\2209170380.py:5: DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n",
      "  ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\naeko\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 123.91it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'paddle.fluid.libpaddle.AnalysisConfig' object has no attribute 'set_optimization_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Inisialisasi model PaddleOCR\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ocr = \u001b[43mPaddleOCR\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_angle_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mocr_paddle_text\u001b[39m(image_path):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Menggunakan PaddleOCR untuk ekstrak teks dari gambar\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddleocr\\_pipelines\\ocr.py:161\u001b[39m, in \u001b[36mPaddleOCR.__init__\u001b[39m\u001b[34m(self, doc_orientation_classify_model_name, doc_orientation_classify_model_dir, doc_unwarping_model_name, doc_unwarping_model_dir, text_detection_model_name, text_detection_model_dir, textline_orientation_model_name, textline_orientation_model_dir, textline_orientation_batch_size, text_recognition_model_name, text_recognition_model_dir, text_recognition_batch_size, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_det_input_shape, text_rec_score_thresh, text_rec_input_shape, lang, ocr_version, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m         base_params[name] = val\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m._params = params\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbase_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddleocr\\_pipelines\\base.py:66\u001b[39m, in \u001b[36mPaddleXPipelineWrapper.__init__\u001b[39m\u001b[34m(self, paddlex_config, **common_args)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._common_args = parse_common_args(\n\u001b[32m     63\u001b[39m     common_args, default_enable_hpi=_DEFAULT_ENABLE_HPI\n\u001b[32m     64\u001b[39m )\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m._merged_paddlex_config = \u001b[38;5;28mself\u001b[39m._get_merged_paddlex_config()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28mself\u001b[39m.paddlex_pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_paddlex_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddleocr\\_pipelines\\base.py:100\u001b[39m, in \u001b[36mPaddleXPipelineWrapper._create_paddlex_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_paddlex_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     99\u001b[39m     kwargs = prepare_common_init_args(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._common_args)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merged_paddlex_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\__init__.py:166\u001b[39m, in \u001b[36mcreate_pipeline\u001b[39m\u001b[34m(pipeline, config, device, pp_option, use_hpip, hpi_config, *args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     config.pop(\u001b[33m\"\u001b[39m\u001b[33mhpi_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m pipeline = \u001b[43mBasePipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\utils\\deps.py:195\u001b[39m, in \u001b[36mpipeline_requires_extra.<locals>._deco.<locals>._wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(pipeline_cls.\u001b[34m__init__\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    194\u001b[39m     require_extra(extra, obj_name=pipeline_name)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_init_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\_parallel.py:103\u001b[39m, in \u001b[36mAutoParallelSimpleInferencePipeline.__init__\u001b[39m\u001b[34m(self, config, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28mself\u001b[39m._executor = MultiDeviceSimpleInferenceExecutor(\n\u001b[32m     98\u001b[39m             \u001b[38;5;28mself\u001b[39m._pipelines,\n\u001b[32m     99\u001b[39m             batch_sampler,\n\u001b[32m    100\u001b[39m             postprocess_result=\u001b[38;5;28mself\u001b[39m._postprocess_result,\n\u001b[32m    101\u001b[39m         )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_device_inference:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28mself\u001b[39m._pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_internal_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\_parallel.py:158\u001b[39m, in \u001b[36mAutoParallelImageSimpleInferencePipeline._create_internal_pipeline\u001b[39m\u001b[34m(self, config, device)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_internal_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, device):\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pipeline_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\ocr\\pipeline.py:73\u001b[39m, in \u001b[36m_OCRPipeline.__init__\u001b[39m\u001b[34m(self, config, device, pp_option, use_hpip, hpi_config)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_doc_preprocessor:\n\u001b[32m     67\u001b[39m     doc_preprocessor_config = config.get(\u001b[33m\"\u001b[39m\u001b[33mSubPipelines\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDocPreprocessor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     69\u001b[39m         {\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpipeline_config_error\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mconfig error for doc_preprocessor_pipeline!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         },\n\u001b[32m     72\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28mself\u001b[39m.doc_preprocessor_pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_preprocessor_config\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mself\u001b[39m.use_textline_orientation = config.get(\u001b[33m\"\u001b[39m\u001b[33muse_textline_orientation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_textline_orientation:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\base.py:140\u001b[39m, in \u001b[36mBasePipeline.create_pipeline\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    137\u001b[39m     hpi_config = hpi_config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    138\u001b[39m     hpi_config = {**\u001b[38;5;28mself\u001b[39m.hpi_config, **hpi_config}\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m pipeline = \u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpp_option\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\__init__.py:166\u001b[39m, in \u001b[36mcreate_pipeline\u001b[39m\u001b[34m(pipeline, config, device, pp_option, use_hpip, hpi_config, *args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     config.pop(\u001b[33m\"\u001b[39m\u001b[33mhpi_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m pipeline = \u001b[43mBasePipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\utils\\deps.py:195\u001b[39m, in \u001b[36mpipeline_requires_extra.<locals>._deco.<locals>._wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(pipeline_cls.\u001b[34m__init__\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    194\u001b[39m     require_extra(extra, obj_name=pipeline_name)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_init_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\_parallel.py:103\u001b[39m, in \u001b[36mAutoParallelSimpleInferencePipeline.__init__\u001b[39m\u001b[34m(self, config, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28mself\u001b[39m._executor = MultiDeviceSimpleInferenceExecutor(\n\u001b[32m     98\u001b[39m             \u001b[38;5;28mself\u001b[39m._pipelines,\n\u001b[32m     99\u001b[39m             batch_sampler,\n\u001b[32m    100\u001b[39m             postprocess_result=\u001b[38;5;28mself\u001b[39m._postprocess_result,\n\u001b[32m    101\u001b[39m         )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_device_inference:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28mself\u001b[39m._pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_internal_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\_parallel.py:158\u001b[39m, in \u001b[36mAutoParallelImageSimpleInferencePipeline._create_internal_pipeline\u001b[39m\u001b[34m(self, config, device)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_internal_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, device):\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pipeline_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\doc_preprocessor\\pipeline.py:67\u001b[39m, in \u001b[36m_DocPreprocessorPipeline.__init__\u001b[39m\u001b[34m(self, config, device, pp_option, use_hpip, hpi_config)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_doc_orientation_classify:\n\u001b[32m     63\u001b[39m     doc_ori_classify_config = config.get(\u001b[33m\"\u001b[39m\u001b[33mSubModules\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\n\u001b[32m     64\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDocOrientationClassify\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mmodel_config_error\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mconfig error for doc_ori_classify_model!\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     66\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28mself\u001b[39m.doc_ori_classify_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_ori_classify_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.use_doc_unwarping = config.get(\u001b[33m\"\u001b[39m\u001b[33muse_doc_unwarping\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_doc_unwarping:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\pipelines\\base.py:107\u001b[39m, in \u001b[36mBasePipeline.create_model\u001b[39m\u001b[34m(self, config, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    105\u001b[39m     pp_option = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m model = \u001b[43mcreate_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\models\\__init__.py:77\u001b[39m, in \u001b[36mcreate_predictor\u001b[39m\u001b[34m(model_name, model_dir, device, pp_option, use_hpip, hpi_config, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m config = BasePredictor.load_config(model_dir)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     75\u001b[39m     model_name == config[\u001b[33m\"\u001b[39m\u001b[33mGlobal\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     76\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel name mismatch，please input the correct model dir.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBasePredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\models\\image_classification\\predictor.py:49\u001b[39m, in \u001b[36mClasPredictor.__init__\u001b[39m\u001b[34m(self, topk, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.topk = topk\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28mself\u001b[39m.preprocessors, \u001b[38;5;28mself\u001b[39m.infer, \u001b[38;5;28mself\u001b[39m.postprocessors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\models\\image_classification\\predictor.py:82\u001b[39m, in \u001b[36mClasPredictor._build\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m     preprocessors[name] = op\n\u001b[32m     80\u001b[39m preprocessors[\u001b[33m\"\u001b[39m\u001b[33mToBatch\u001b[39m\u001b[33m\"\u001b[39m] = ToBatch()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m infer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_static_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m postprocessors = {}\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mPostProcess\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\models\\base\\predictor\\base_predictor.py:240\u001b[39m, in \u001b[36mBasePredictor.create_static_infer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_static_infer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_hpip:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPaddleInfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mMODEL_FILE_PREFIX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pp_option\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m HPInfer(\n\u001b[32m    243\u001b[39m             \u001b[38;5;28mself\u001b[39m.model_dir,\n\u001b[32m    244\u001b[39m             \u001b[38;5;28mself\u001b[39m.MODEL_FILE_PREFIX,\n\u001b[32m    245\u001b[39m             \u001b[38;5;28mself\u001b[39m._hpi_config,\n\u001b[32m    246\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\models\\common\\static_infer.py:274\u001b[39m, in \u001b[36mPaddleInfer.__init__\u001b[39m\u001b[34m(self, model_dir, model_file_prefix, option)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.model_file_prefix = model_file_prefix\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m._option = option\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28mself\u001b[39m.predictor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m.infer = PaddleInferChainLegacy(\u001b[38;5;28mself\u001b[39m.predictor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pkl\\paddleocr\\Lib\\site-packages\\paddlex\\inference\\models\\common\\static_infer.py:423\u001b[39m, in \u001b[36mPaddleInfer._create\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33menable_new_executor\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    422\u001b[39m             config.enable_new_executor()\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m         \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_optimization_level\u001b[49m(\u001b[32m3\u001b[39m)\n\u001b[32m    425\u001b[39m config.enable_memory_optim()\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m del_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._option.delete_pass:\n",
      "\u001b[31mAttributeError\u001b[39m: 'paddle.fluid.libpaddle.AnalysisConfig' object has no attribute 'set_optimization_level'"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import re\n",
    "\n",
    "# Inisialisasi model PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "def ocr_paddle_text(image_path):\n",
    "    \"\"\"Menggunakan PaddleOCR untuk ekstrak teks dari gambar\"\"\"\n",
    "    result = ocr.ocr(image_path, cls=True)\n",
    "    lines = []\n",
    "    for line in result[0]:\n",
    "        lines.append(line[1][0])  # ambil teks saja\n",
    "    return lines\n",
    "\n",
    "def extract_items_from_lines(lines):\n",
    "    \"\"\"Parsing daftar belanja dari hasil OCR lines\"\"\"\n",
    "    items = []\n",
    "    pattern = re.compile(r'(?P<name>.+?)\\s+(?P<qty>\\d+)\\s+(?P<price>\\d+[.,]?\\d*)\\s+(?P<total>\\d+[.,]?\\d*)$')\n",
    "\n",
    "    for line in lines:\n",
    "        match = pattern.match(line.strip())\n",
    "        if match:\n",
    "            nama = match.group(\"name\").strip()\n",
    "            qty = float(match.group(\"qty\"))\n",
    "            price = float(match.group(\"price\").replace(',', '.'))\n",
    "            subtotal = float(match.group(\"total\").replace(',', '.'))\n",
    "            items.append({\n",
    "                \"nama\": nama,\n",
    "                \"qty\": qty,\n",
    "                \"harga_satuan\": price,\n",
    "                \"subtotal\": subtotal\n",
    "            })\n",
    "    return items\n",
    "\n",
    "# === CONTOH PEMAKAIAN ===\n",
    "if __name__ == \"__main__\":\n",
    "    path_gambar = \"sample/Receipt NIVEA/ini.jpg\"\n",
    "    lines = ocr_paddle_text(path_gambar)\n",
    "    daftar_belanja = extract_items_from_lines(lines)\n",
    "\n",
    "    print(\"Hasil Deteksi Produk dari Struk:\")\n",
    "    for item in daftar_belanja:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdd181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
